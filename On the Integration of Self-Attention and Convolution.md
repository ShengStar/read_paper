# On the Integration of Self-Attention and Convolution
## 摘要
卷积和自我注意是表征学习的两种强大技术，它们通常被认为是两种不同的对等方法。在本文中，我们证明了它们之间存在着很强的内在联系，即这两种范式的大部分计算实际上是用相同的操作完成的。具体来说，我们首先证明了核大小为k的传统卷积可以分解成k*k个单独的1*1个卷积，然后进行移位和求和操作。然后，我们将self-attention模块中查询、键和值的投影解释为多个1*1*卷积，然后计算注意权值并对这些值进行聚合。因此，两个模块的第一阶段都包含类似的操作。更重要的是，与第二阶段相比，第一阶段贡献了占主导地位的计算复杂度(信道大小的平方)。这一观察结果自然导致了这两种看起来截然不同的范式的优雅集成，即，一个混合模型既享受了自我注意和卷积(ACmix)的好处，同时与纯卷积或自我注意的对等物相比，具有最小的计算开销。大量的实验表明，我们的模型在图像识别和下游任务上取得了持续改进的结果。
## 引言
尽管这两种方法都取得了巨大的成功，但卷积和自我关注模块通常遵循不同的设计范式。传统的卷积根据卷积滤波器的权值在整个特征映射中共享，在局部接受域上利用一个聚合函数。其内在特性对图像处理产生了至关重要的诱导偏差。相比之下，自我注意模块采用基于输入特征上下文的加权平均操作，通过相关像素对之间的相似函数动态计算注意权值。这种灵活性使注意力模块能够自适应地聚焦于不同的区域，并捕获更多的信息特征。

考虑到卷积和自我注意的不同和互补性质，通过集成这些模块，存在从这两种范式中获益的潜在可能性。先前的工作已经从几个不同的角度探讨了自我注意和卷积的结合。早期的研究，如SENet [23]， CBAM[47]，表明自我注意机制可以作为卷积模块的一种扩充。最近，在CNN模型中，自我注意模块被提出作为单独的块来替代传统的卷积，例如SAN [54]， BoTNet[41]。另一种研究方向是将self-attention和convolution结合在一个单独的block中，如AA-ResNet [3]， Container[17]，而这种架构在为每个模块设计独立路径方面存在局限性。因此，现有的方法仍然将自我注意和卷积视为不同的部分，它们之间的潜在关系没有得到充分利用。

在这篇论文中，我们试图揭示自我注意和卷积之间更紧密的关系。通过对这两个模块的运算进行分解，我们发现它们在很大程度上依赖于相同的1*1卷积运算。基于这一观察，我们开发了一个混合模型，命名为ACmix，并以最小的计算开销优雅地集成了自我关注和卷积。具体来说，我们首先对输入的特征映射进行1*1卷积投影，得到一组丰富的中间特征。然后，中间特征按照不同的范式分别以自我关注和卷积的方式进行重用和聚合。通过这种方式，ACmix享受了这两个模块的好处，并有效地避免了两次执行昂贵的投影操作。

综上所述，我们的贡献主要体现在两个方面:(1)揭示了自我注意和卷积之间的内在联系，为理解这两个模块之间的联系提供了新的视角，并为设计新的学习范式提供了灵感。(2)提出了一种优雅的自关注和卷积模块的集成，它享受了这两个世界的好处。经验证据表明，混合模型的性能优于纯卷积模型或自注意模型。

## 相关工作

先前提出的多种图像注意机制表明，它可以克服卷积网络局部性的局限性。因此，许多研究者探索了利用注意模块或利用更多的关系信息来增强卷积网络功能的可能性。特别是，挤压和激发(SE)和聚集-激发(GE)重新权衡每个通道的地图。BAM和CBAM分别对通道和空间位置进行重新加权，以更好地细化feature map。AA-Resnet通过连接来自另一个独立的自我注意管道的注意图来增强某些卷积层。BoTNet在模型的后期阶段用自我关注模块替代卷积。一些工作旨在设计一个更灵活的特征提取器，从更广泛的像素聚集信息。Hu等人提出了一种基于局部像素组成关系自适应确定聚合权值的局部关系方法。Wang等人提出了非局域网络，通过引入非局域块来比较全局像素之间的相似性，从而增加接受域。

随着Vision Transformer的出现，许多基于Transformer的变体被提出，并在计算机视觉任务方面取得了显著的改进。其中，已有的研究主要是在变压器模型中加入卷积运算以引入额外的感应偏差。CvT在标记化过程中采用卷积，利用stride卷积来降低自我注意的计算复杂度。ViT with convolutional stem提出在早期增加convolutional，以达到更稳定的训练。CSwin Transformer采用了基于卷积的位置编码技术，并对下游任务进行了改进。Conformer将Transformer与一个独立的CNN模型结合起来，从而集成了这两种特性。

## 卷积


